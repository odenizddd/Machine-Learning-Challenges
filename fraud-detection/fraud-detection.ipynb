{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset from https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "value_counts = data.value_counts('Class')\n",
    "print(\"There are {} frauds and {} non-frauds in this dataset.\".format(value_counts[1], value_counts[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the 'Time' column\n",
    "\n",
    "data = data.drop('Time', axis=1)\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"There are {} training samples and {} test samples.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "print(y_train.value_counts('Class'))\n",
    "print(y_test.value_counts('Class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier and fit it to the training data\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier on the test data\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Baseline Scores For Decision Tree Classifier\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I shall experiment with different techniques that might improve the performance of the model (or several models) and determine which of them really works.\n",
    "\n",
    "The techniques I want to try are as follows:\n",
    "\n",
    "1. Scaling features that were not already scaled.\n",
    "2. Balancing training data by oversampling with SMOTE.\n",
    "3. Balancing training data with random undersampling.\n",
    "4. Removing outliers from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the amount.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = X_train.copy()\n",
    "\n",
    "X_train_std['Amount'] = scaler.fit_transform(X_train_std[['Amount']])\n",
    "\n",
    "X_test_std = X_test.copy()\n",
    "\n",
    "X_test_std['Amount'] = scaler.transform(X_test_std[['Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance after scaling the amount.\n",
    "\n",
    "clf_std = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf_std.fit(X_train_std, y_train)\n",
    "\n",
    "clf_std_pred = clf_std.predict(X_test_std)\n",
    "\n",
    "print(\"Scores For Decision Tree Classifier After Scaling The Amount\")\n",
    "\n",
    "print(classification_report(y_test, clf_std_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't get any improvements after normalizing amount.\n",
    "\n",
    "Let's continue with oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new training dataset with SMOTE.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_std, y_train)\n",
    "\n",
    "print(y_train_smote.value_counts('Class'))\n",
    "\n",
    "print(\"There are {} training samples and {} test samples.\".format(X_train_smote.shape[0], X_test_std.shape[0]))\n",
    "\n",
    "# Test the performance after using SMOTE.\n",
    "\n",
    "clf_smote = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "clf_smote_pred = clf_smote.predict(X_test_std)\n",
    "\n",
    "print(\"Scores For Decision Tree Classifier After Using SMOTE\")\n",
    "\n",
    "print(classification_report(y_test, clf_smote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling with SMOTE produced even worse results. That's unexpected. I thought balancing training classes would improve the score.\n",
    "\n",
    "Let's try random undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new training dataset with NearMiss.\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nearmiss = NearMiss()\n",
    "\n",
    "X_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(X_train_std, y_train)\n",
    "\n",
    "print(y_train_nearmiss.value_counts('Class'))\n",
    "\n",
    "print(\"There are {} training samples and {} test samples.\".format(X_train_nearmiss.shape[0], X_test_std.shape[0]))\n",
    "\n",
    "# Test the performance after using NearMiss.\n",
    "\n",
    "clf_nearmiss = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf_nearmiss.fit(X_train_nearmiss, y_train_nearmiss)\n",
    "\n",
    "clf_nearmiss_pred = clf_nearmiss.predict(X_test_std)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "\n",
    "print(\"Scores For Decision Tree Classifier After Using NearMiss\")\n",
    "\n",
    "print(classification_report(y_test, clf_nearmiss_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling wrecks the scores. Let's try training multiple decision tree classifiers and train them on different random subsamples and take a majority vote to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([X_train_std, y_train], axis=1)\n",
    "\n",
    "frauds = training_data[training_data['Class'] == 1]\n",
    "non_frauds = training_data[training_data['Class'] == 0]\n",
    "\n",
    "fraud_count = len(frauds)\n",
    "\n",
    "def get_random_subsample(frauds, non_frauds, fraud_count):\n",
    "    non_frauds_sample = non_frauds.sample(fraud_count)\n",
    "\n",
    "    balanced_training_data = pd.concat([frauds, non_frauds_sample])\n",
    "    balanced_training_data = balanced_training_data.sample(frac=1)\n",
    "\n",
    "    X_train_balanced = balanced_training_data.drop('Class', axis=1)\n",
    "    y_train_balanced = balanced_training_data['Class']\n",
    "\n",
    "    X_train_balanced.reset_index(drop=True, inplace=True)\n",
    "    y_train_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return X_train_balanced, y_train_balanced.astype('int')\n",
    "\n",
    "tree_ensemble = []\n",
    "\n",
    "for i in range(100):\n",
    "    X_train_balanced, y_train_balanced = get_random_subsample(frauds, non_frauds, fraud_count)\n",
    "\n",
    "    clf_balanced = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    clf_balanced.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "    tree_ensemble.append(clf_balanced)\n",
    "\n",
    "def predict(X):\n",
    "    return sum(clf_balanced.predict(X) for clf_balanced in tree_ensemble) / len(tree_ensemble)\n",
    "\n",
    "y_pred_balanced = predict(X_test_std)\n",
    "\n",
    "y_pred_balanced = (y_pred_balanced > 0.5).astype(int)\n",
    "\n",
    "print(\"Scores For Decision Tree Classifier After Using Balanced Training Data\")\n",
    "\n",
    "print(classification_report(y_test, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an ensemble of tree on different subsets of data also doesn't work.\n",
    "\n",
    "Let's see how removing outliers affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the training data.\n",
    "\n",
    "# Q1 and Q3 are the first and third quartiles of the 'Amount' column.\n",
    "\n",
    "Q1 = X_train_std['Amount'].quantile(0.25)\n",
    "\n",
    "Q3 = X_train_std['Amount'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# The lower and upper bounds are the values that are 1.5 times the IQR below Q1 and above Q3.\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove the outliers from the training data.\n",
    "\n",
    "X_train_no_outliers = X_train_std[(X_train_std['Amount'] > lower_bound) & (X_train_std['Amount'] < upper_bound)]\n",
    "\n",
    "y_train_no_outliers = y_train.loc[X_train_no_outliers.index]\n",
    "\n",
    "print(\"There are {} training samples and {} test samples.\".format(X_train_no_outliers.shape[0], X_test_std.shape[0]))\n",
    "\n",
    "# Test the performance after removing the outliers.\n",
    "\n",
    "clf_no_outliers = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf_no_outliers.fit(X_train_no_outliers, y_train_no_outliers)\n",
    "\n",
    "clf_no_outliers_pred = clf_no_outliers.predict(X_test_std)\n",
    "\n",
    "print(\"Scores For Decision Tree Classifier After Removing Outliers\")\n",
    "\n",
    "print(classification_report(y_test, clf_no_outliers_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the outliers from the Amount column slightly improves the model. I wonder if removing the outliers from other columns would help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_from_column(X, y, column):\n",
    "\n",
    "    Q1 = X[column].quantile(0.25)\n",
    "\n",
    "    Q3 = X[column].quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    X_train_no_outliers = X[(X[column] > lower_bound) & (X[column] < upper_bound)]\n",
    "\n",
    "    y_train_no_outliers = y.loc[X_train_no_outliers.index]\n",
    "\n",
    "    return X_train_no_outliers, y_train_no_outliers\n",
    "\n",
    "def remove_outliers_from_columns(X, y, columns):\n",
    "    X_train_no_outliers, y_train_no_outliers = X, y\n",
    "\n",
    "    for column in columns:\n",
    "        X_train_no_outliers, y_train_no_outliers = remove_outliers_from_column(X_train_no_outliers, y_train_no_outliers, column)\n",
    "\n",
    "    return X_train_no_outliers, y_train_no_outliers\n",
    "\n",
    "# X_train_no_outliers, y_train_no_outliers = remove_outliers_from_column(X_train_std, y_train, 'Amount')\n",
    "# X_train_no_outliers, y_train_no_outliers = remove_outliers_from_column(X_train_no_outliers, y_train_no_outliers, 'V17')\n",
    "\n",
    "X_train_no_outliers, y_train_no_outliers = remove_outliers_from_columns(X_train_std, y_train, ['Amount'])\n",
    "\n",
    "print(\"There are {} training samples and {} test samples.\".format(X_train_no_outliers.shape[0], X_test_std.shape[0]))\n",
    "\n",
    "# Test the performance after removing the outliers from the 'Amount' column.\n",
    "\n",
    "clf_no_outliers = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf_no_outliers.fit(X_train_no_outliers, y_train_no_outliers)\n",
    "\n",
    "clf_no_outliers_pred = clf_no_outliers.predict(X_test_std)\n",
    "\n",
    "print(\"Scores For Decision Tree Classifier After Removing Outliers From The 'Amount' Column\")\n",
    "\n",
    "print(classification_report(y_test, clf_no_outliers_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers in the amount column look promising. Other columns not so much. Why? When does it help to remove outliers from a columns. Anyways, let's train a random forest on this new data without outliers in the amount column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance after removing the outliers from the 'Amount' on a random forest.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_no_outliers_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf_no_outliers_forest.fit(X_train_no_outliers, y_train_no_outliers)\n",
    "\n",
    "clf_no_outliers_forest_pred = clf_no_outliers_forest.predict(X_test_std)\n",
    "\n",
    "print(\"Scores For Random Forest Classifier After Removing Outliers From The 'Amount' Column\")\n",
    "\n",
    "print(classification_report(y_test, clf_no_outliers_forest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with the original dataset.add\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Scores For Random Forest Classifier With Original Dataset\")\n",
    "\n",
    "print(classification_report(y_test, clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, clf_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, clf_no_outliers_forest_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the outliers doesn't improve the random forest classifier all that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the notebook:\n",
    "\n",
    "Scaling the data, oversampling or undersampling doesn't really help with model performance. Only removing outliers does help a little to decision tree classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
